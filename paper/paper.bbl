\begin{thebibliography}{31}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Agirre et~al.(2012)Agirre, Cer, Diab, and
  Gonzalez-Agirre}]{Agirre2012SemEval2012T6}
Eneko Agirre, Daniel~Matthew Cer, Mona~T. Diab, and Aitor Gonzalez-Agirre.
  2012.
\newblock Semeval-2012 task 6: A pilot on semantic textual similarity.
\newblock In \emph{*SEMEVAL}.

\bibitem[{Ainslie et~al.(2020)Ainslie, Onta{\~n}{\'o}n, Alberti, Cvicek,
  Fisher, Pham, Ravula, Sanghai, Wang, and Yang}]{Ainslie2020ETCEL}
Joshua Ainslie, Santiago Onta{\~n}{\'o}n, Chris Alberti, Vaclav Cvicek,
  Zachary~Kenneth Fisher, Philip Pham, Anirudh Ravula, Sumit~K. Sanghai, Qifan
  Wang, and Li~Yang. 2020.
\newblock Etc: Encoding long and structured inputs in transformers.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}.

\bibitem[{Beltagy et~al.(2020)Beltagy, Peters, and
  Cohan}]{Beltagy2020LongformerTL}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan. 2020.
\newblock Longformer: The long-document transformer.
\newblock \emph{ArXiv}, abs/2004.05150.

\bibitem[{Carlsson et~al.(2021)Carlsson, Gyllensten, Gogoulou, Hellqvist, and
  Sahlgren}]{Carlsson2021SemanticRW}
Fredrik Carlsson, Amaru~Cuba Gyllensten, Evangelia Gogoulou,
  Erik~Ylip{\"a}{\"a} Hellqvist, and Magnus Sahlgren. 2021.
\newblock Semantic re-tuning with contrastive tension.
\newblock In \emph{ICLR}.

\bibitem[{Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton}]{Chen2020ASF}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey~E. Hinton. 2020.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock \emph{ArXiv}, abs/2002.05709.

\bibitem[{Conneau and Kiela(2018)}]{Conneau2018SentEvalAE}
Alexis Conneau and Douwe Kiela. 2018.
\newblock Senteval: An evaluation toolkit for universal sentence
  representations.
\newblock \emph{ArXiv}, abs/1803.05449.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{Devlin2019BERTPO}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}.

\bibitem[{Ding et~al.(2020)Ding, Zhou, Yang, and Tang}]{Ding2020CogLTXAB}
Ming Ding, Chang Zhou, Hongxia Yang, and Jie Tang. 2020.
\newblock Cogltx: Applying bert to long texts.
\newblock In \emph{NeurIPS}.

\bibitem[{Fang et~al.(2019)Fang, Sun, Gan, Pillai, Wang, and
  Liu}]{Fang2019HierarchicalGN}
Yuwei Fang, S.~Sun, Zhe Gan, Rohit~Radhakrishna Pillai, Shuohang Wang, and
  Jingjing Liu. 2019.
\newblock Hierarchical graph network for multi-hop question answering.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}.

\bibitem[{Gao et~al.(2021)Gao, Yao, and Chen}]{Gao2021SimCSESC}
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
\newblock Simcse: Simple contrastive learning of sentence embeddings.
\newblock \emph{ArXiv}, abs/2104.08821.

\bibitem[{Gidiotis and Tsoumakas(2020)}]{Gidiotis2020ADA}
Alexios Gidiotis and Grigorios Tsoumakas. 2020.
\newblock A divide-and-conquer approach to the summarization of long documents.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 28:3029--3040.

\bibitem[{Grail(2021)}]{Grail2021GlobalizingBT}
Quentin Grail. 2021.
\newblock Globalizing bert-based transformer architectures for long document
  summarization.
\newblock In \emph{Conference of the European Chapter of the Association for
  Computational Linguistics}.

\bibitem[{Hadsell et~al.(2006)Hadsell, Chopra, and
  LeCun}]{Hadsell2006DimensionalityRB}
Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006.
\newblock Dimensionality reduction by learning an invariant mapping.
\newblock \emph{2006 IEEE Computer Society Conference on Computer Vision and
  Pattern Recognition (CVPR'06)}, 2:1735--1742.

\bibitem[{Hill et~al.(2016)Hill, Cho, and Korhonen}]{Hill2016LearningDR}
Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
\newblock Learning distributed representations of sentences from unlabelled
  data.
\newblock In \emph{NAACL}.

\bibitem[{Huang et~al.(2021)Huang, Cao, Parulian, Ji, and
  Wang}]{Huang2021EfficientAF}
Luyang Huang, Shuyang Cao, Nikolaus~Nova Parulian, Heng Ji, and Lu~Wang. 2021.
\newblock Efficient attentions for long document summarization.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics}.

\bibitem[{Karpukhin et~al.(2020)Karpukhin, Oğuz, Min, Lewis, Wu, Edunov, Chen,
  and tau Yih}]{Karpukhin2020DensePR}
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell~Yu Wu,
  Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020.
\newblock Dense passage retrieval for open-domain question answering.
\newblock \emph{ArXiv}, abs/2004.04906.

\bibitem[{Kingma and Ba(2014)}]{Kingma2014AdamAM}
Diederik~P. Kingma and Jimmy Ba. 2014.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{CoRR}, abs/1412.6980.

\bibitem[{Kiros et~al.(2015)Kiros, Zhu, Salakhutdinov, Zemel, Urtasun,
  Torralba, and Fidler}]{Kiros2015SkipThoughtV}
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard~S. Zemel, Raquel Urtasun,
  Antonio Torralba, and Sanja Fidler. 2015.
\newblock Skip-thought vectors.
\newblock \emph{ArXiv}, abs/1506.06726.

\bibitem[{Lang(1995)}]{Lang1995NewsWeederLT}
Ken Lang. 1995.
\newblock Newsweeder: Learning to filter netnews.
\newblock In \emph{ICML}.

\bibitem[{Li et~al.(2022)Li, Shang, and McAuley}]{Li2022UCTopicUC}
Jiacheng Li, Jingbo Shang, and Julian McAuley. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.426} {{UCT}opic:
  Unsupervised contrastive learning for phrase representations and topic
  mining}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 6159--6169,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{Liu2019RoBERTaAR}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{ArXiv}, abs/1907.11692.

\bibitem[{Logeswaran and Lee(2018)}]{Logeswaran2018AnEF}
Lajanugen Logeswaran and Honglak Lee. 2018.
\newblock An efficient framework for learning sentence representations.
\newblock \emph{ArXiv}, abs/1803.02893.

\bibitem[{Martin and Johnson(2015)}]{martin-johnson-2015-efficient}
Fiona Martin and Mark Johnson. 2015.
\newblock \href {https://aclanthology.org/U15-1013} {More efficient topic
  modelling through a noun only approach}.
\newblock In \emph{Proceedings of the Australasian Language Technology
  Association Workshop 2015}, pages 111--115, Parramatta, Australia.

\bibitem[{Meng et~al.(2021)Meng, Xiong, Bajaj, Tiwary, Bennett, Han, and
  Song}]{Meng2021COCOLMCA}
Yu~Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han,
  and Xia Song. 2021.
\newblock Coco-lm: Correcting and contrasting text sequences for language model
  pretraining.
\newblock \emph{ArXiv}, abs/2102.08473.

\bibitem[{Pappagari et~al.(2019)Pappagari, Żelasko, Villalba, Carmiel, and
  Dehak}]{Pappagari2019HierarchicalTF}
R.~Pappagari, Piotr Żelasko, Jes{\'u}s Villalba, Yishay Carmiel, and Najim
  Dehak. 2019.
\newblock Hierarchical transformers for long document classification.
\newblock \emph{2019 IEEE Automatic Speech Recognition and Understanding
  Workshop (ASRU)}, pages 838--844.

\bibitem[{Reimers et~al.(2016)Reimers, Beyer, and
  Gurevych}]{Reimers2016TaskOrientedIE}
Nils Reimers, Philip Beyer, and Iryna Gurevych. 2016.
\newblock Task-oriented intrinsic evaluation of semantic textual similarity.
\newblock In \emph{COLING}.

\bibitem[{Rohde et~al.(2021)Rohde, Wu, and Liu}]{Rohde2021HierarchicalLF}
Tobias Rohde, Xiaoxia Wu, and Yinhan Liu. 2021.
\newblock Hierarchical learning for generation with long source sequences.
\newblock \emph{ArXiv}, abs/2104.07545.

\bibitem[{Sandhaus(2008)}]{sandhaus2008new}
Evan Sandhaus. 2008.
\newblock The new york times annotated corpus.
\newblock \emph{Linguistic Data Consortium, Philadelphia}, 6(12):e26752.

\bibitem[{Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Onta{\~n}{\'o}n, Pham, Ravula, Wang, Yang, and Ahmed}]{Zaheer2020BigBT}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Onta{\~n}{\'o}n, Philip Pham, Anirudh Ravula, Qifan Wang,
  Li~Yang, and Amr Ahmed. 2020.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{ArXiv}, abs/2007.14062.

\bibitem[{Zhang et~al.(2020)Zhang, He, Liu, Lim, and Bing}]{Zhang2020AnUS}
Yan Zhang, Ruidan He, Zuozhu Liu, Kwan~Hui Lim, and Lidong Bing. 2020.
\newblock An unsupervised sentence embedding method by mutual information
  maximization.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}.

\bibitem[{Zhong et~al.(2021)Zhong, Liu, Xu, Zhu, and
  Zeng}]{Zhong2021DialogLMPM}
Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. 2021.
\newblock Dialoglm: Pre-trained model for long dialogue understanding and
  summarization.
\newblock In \emph{AAAI Conference on Artificial Intelligence}.

\end{thebibliography}
